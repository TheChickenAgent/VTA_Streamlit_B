# streamlit run app8B.py
# Version review:
# - Open QA chat: o4-mini
# - True/False questions: o4-mini (only generation & help)

__import__('pysqlite3')
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')

import numpy as np
import requests
import streamlit as st
import os
#from dotenv import load_dotenv
from openai import OpenAI
from openai.types.responses.response_output_message import ResponseOutputMessage
#from google import genai
import io
from fpdf import FPDF
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma
import time
#from google.genai import types
import re
import pickle
from datetime import datetime





# Load environment variables from a .env file
#load_dotenv()
OPENAI_API = st.secrets.get("OPENAI_API_KEY")
#OPENAI_API = os.getenv('OPENAI_API_KEY')
#GOOGLE_API = os.getenv('GOOGLE_API_KEY')
embedding = OpenAIEmbeddings(model="text-embedding-3-large", openai_api_key=OPENAI_API)
db_openai = Chroma(persist_directory="./vectordb/openai_vectorDB/", embedding_function=embedding) #for existing database
MODEL_QUESTION_GENERATION = "o4-mini"  # Model for question generation
#MODEL_QUESTION_GENERATION = "gpt-3.5-turbo"  # Model for question generation


def get_page_numbers(page_numbers: list[str]) -> list[str]:
    """
    Get the page numbers from the metadata.

    Args:
        page_numbers (list): list of page numbers

    Returns:
        list: sorted list of page numbers
    """
    int_page_numbers = []
    for page_number in page_numbers:
        if "and" in page_number:
            page_number = page_number.split(" and ")
            page_number = [int(i) for i in page_number]
            int_page_numbers.extend(page_number)
        else:
            page_number = int(page_number)
            int_page_numbers.append(page_number)

    # Unique the page numbers
    int_page_numbers = list(set(int_page_numbers))

    # Sort the page numbers
    int_page_numbers.sort()

    # Now we need to string them again
    return [str(i) for i in int_page_numbers]


def intro():
    st.write("# Virtual Teaching Assistant for Linear Algebra.üëã :mortar_board: :computer: :books: :school:")
    st.write("## Application type: B")
    st.markdown("""
                - Open QA chat: o4-mini
                - True/False questions: o4-mini
                """)
    st.write("## :warning: THIS TOOL IS UNDER DEVELOPMENT :construction_worker:")
    #st.sidebar.success("Select a practice mode.")

    st.markdown(
        """
        This app is a virtual teaching assistant for linear algebra, powered by an LLM.
        It is designed to help students learn linear algebra concepts, like:
        - solving linear systems of equations;
        - matrix algebra;
        - determinants;
        - eigenvalues and eigenvectors;
        through the use of a chat interface and true/false questions.
    """
    )

def stream_string_data(text: str):
    for word in text.split(" "):
        yield word + " "
        time.sleep(0.02)

def chat_TF_generation(help_TF_Q: dict, idx: int):
    st.markdown("## Help chat")
    st.markdown("This chat interface allows you to ask questions about the True/False statement generated by the system. Please note that this window will dissapear when you submit your answer to the question.")

    llm = OpenAI(api_key=OPENAI_API)
    #genai_client = genai.Client(api_key=GOOGLE_API)

    if "openai_model" not in st.session_state:
        st.session_state["openai_model"] = "o4-mini"
        #st.session_state["openai_model"] = "gpt-3.5-turbo"
    if "container_id" not in st.session_state:
        print("Creating container for Code Interpreter...")
        cont = llm.containers.create(name="TF_qa_chat")
        container_id_manual = cont.id
        st.session_state["container_id"] = container_id_manual

    if f"messages_{idx}" not in st.session_state:
        st.session_state[f"messages_{idx}"] = []

    if f"init_chat_TF_{idx}" not in st.session_state:
        st.session_state[f"init_chat_TF_{idx}"] = True

    for message in st.session_state[f"messages_{idx}"]:
        if message["role"] == "system":
            continue  # Skip system messages in the chat display
        if message["role"] == "references":
            with st.chat_message("references", avatar="üìñ"):
                st.markdown(message["content"])
        else:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])


    question = help_TF_Q.get("question", "")
    explanation = help_TF_Q.get("explanation", "")
    answer = help_TF_Q.get("answer", "")
    #query = None

    if question=="" or explanation=="" or answer=="":
        st.error("Error: The question, explanation, or answer is missing.")
    elif st.session_state[f"init_chat_TF_{idx}"] == True:
        st.session_state[f"init_chat_TF_{idx}"] = False
        print("Starting")
        # Build custom prompt
        custom_prompt = (
            "You are an assistant for question-answering tasks in linear algebra. "
            "The user would like to get clarification on a True/False question. "
            "The question is: " + question + ". "
            "The explanation is: " + explanation + ". "
            "The answer is: " + str(answer) + ". "
            "Please use LaTeX formatting for mathematical expressions by writing them between dollar signs."
            "For example, to write a matrix, use $\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$. "
            "You can write and run code to answer the question. "
            "If you don't know the answer, say that you don't know. "
            "Do not answer the question directly, but rather ask for clarification or additional information if needed. "
        )

        st.session_state[f"messages_{idx}"].append({"role": "system", "content": custom_prompt})

        try:
            response = llm.responses.create(
                model=st.session_state["openai_model"],
                tools=[{"type": "code_interpreter", "container": st.session_state["container_id"]}],
                input=custom_prompt,
            )
            # Extract the answer from the response
            #answer = extract_llm_response_code_interpreter(response.output)
            #if answer == "":
            #    raise ValueError("Empty answer received from LLM.")
        except Exception as e:
            print(f"Error generating questions.\nError: {e}")
            st.error(f"Error generating questions.\nError: {e}")

        
    if query := st.chat_input("How can I help you with this True/False question?"):
        st.session_state[f"messages_{idx}"].append({"role": "user", "content": query})
        with st.chat_message("user"):
            st.markdown(query)
        
        # Retrieve relevant context from the vector database
        query_rag = f"Question: {question}\nExplanation: {explanation}\nAnswer: {answer}" 
        retrieved_docs = db_openai.similarity_search(query_rag, k=4)
        context = "\n\n".join(doc.page_content for doc in retrieved_docs)
        references = get_page_numbers([doc.metadata['page'] for doc in retrieved_docs])
        print(references)

        total_query = f"Use the following pieces of retrieved context to aid the explanation: {context}\n\nUser query: {query}"
        st.session_state[f"messages_{idx}"].append({"role": "system", "content": total_query})

        try:
            response = llm.responses.create(
                model=st.session_state["openai_model"],
                tools=[{"type": "code_interpreter", "container": st.session_state["container_id"]}],
                input=st.session_state[f"messages_{idx}"],
            )
            # Extract the answer from the response
            answer = extract_llm_response_code_interpreter(response.output)
            if answer == "":
                raise ValueError("Empty answer received from LLM.")
        except Exception as e:
            print(f"Error generating questions.\nError: {e}")
            st.error(f"Error generating questions.\nError: {e}")

        #answer = transform_latex_text(str(answer))

        with st.chat_message("assistant"):
            answer = st.write_stream(stream_string_data(str(answer)))
        st.session_state[f"messages_{idx}"].append({"role": "assistant", "content": answer})

        if references:
            with st.chat_message("references", avatar="üìñ"):
                if len(references) == 1:
                    ref = f"Reference: page {references[0]}"
                else:
                    ref = f"References: pages {", ".join(references[:-1])}, and {references[-1]}"
                st.write(ref)
            #st.session_state.messages.append({"role": "assistant", "content": response})
            st.session_state[f"messages_{idx}"].append({"role": "references", "content": ref})

def chat():
    st.title("QA chat for Linear Algebra")
    st.markdown("This chat interface allows you to ask open questions about Linear Algebra. Type 'clear' to reset the chat history.")
    #st.write("The determinant of the matrix $A = \\begin{pmatrix}1 & 2\\\\50 & 10\\end{pmatrix}$ is $\\det(A) = 1\\cdot10 -\\ 2\\cdot50 =\\ 10 - 100 =-90.$")
    
    llm = OpenAI(api_key=OPENAI_API)
    if "openai_model" not in st.session_state:
        st.session_state["openai_model"] = "o4-mini"
    if "container_id" not in st.session_state:
        print("Creating container for Code Interpreter...")
        cont = llm.containers.create(name="open_qa_chat")
        container_id_manual = cont.id
        st.session_state["container_id"] = container_id_manual
    #genai_client = genai.Client(api_key=GOOGLE_API)

    if "messages_chat" not in st.session_state:
        st.session_state.messages_chat = []

    for message in st.session_state.messages_chat:
        if message["role"] == "references":
            with st.chat_message("references", avatar="üìñ"):
                st.markdown(message["content"])
        elif message["role"] == "assistant":
            with st.chat_message("assistant"):
                if message["content"].endswith(("jpg", "jpeg", "png")):
                    st.image(message["content"], caption="Generated image")
                else:
                    st.markdown(message["content"])
        else:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

    if query := st.chat_input("Ask a question about Linear Algebra."):
        if "clear" == query.lower():  
            #print(st.session_state.messages_chat)
            safe_time = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
            with open(f'chats/Open_chat_A_list_{safe_time}.pkl', 'wb') as f:
                pickle.dump(st.session_state.messages_chat, f)
            st.session_state.messages_chat = []
            if "open_chat" in st.session_state:
                del st.session_state["open_chat"]
            st.rerun()
        else:
            st.session_state.messages_chat.append({"role": "user", "content": query})
            with st.chat_message("user"):
                st.markdown(query)
            #print(query)

            # Retrieve relevant context from the vector database
            retrieved_docs = db_openai.similarity_search(query, k=4)
            context = "\n\n".join(doc.page_content for doc in retrieved_docs)
            references = get_page_numbers([doc.metadata['page'] for doc in retrieved_docs])
            print(references)

            # Build custom prompt
            # Combine message history with the custom prompt as system message
            custom_prompt = (
                "You are an assistant for question-answering tasks in linear algebra. "
                "If the question is not related to linear algebra, politely decline to answer it and put at the end of the repsonse REFUSED=TRUE. "
                "Please use LaTeX formatting for mathematical expressions by writing them between dollar signs."
                "For example, to write a matrix, use $\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$. "
                "For example, a determinant of a matrix $A = \\begin{pmatrix}1 & 2\\\\50 & 10\\end{pmatrix}$ is $\\det(A) = 1\\cdot10 -\\ 2\\cdot50 =\\ 10 - 100 =-90.$"
                "If you don't know the answer, say 'I don't know'. "
                "You can write and run code to answer the question. "
                "Use the following pieces of retrieved context to answer the question. "
                "\n\nContext:\n" + context
            )


            # Prepare messages: system prompt + previous messages + latest user query
            messages = [{"role": "system", "content": custom_prompt}]
            messages += [{"role": m["role"], "content": m["content"]} for m in st.session_state.messages_chat if m["role"] != "references"]
            # Overwrite the last user message to ensure it includes the context
            if messages and messages[-1]["role"] == "user":
                messages[-1]["content"] = query

            try:
                response = llm.responses.create(
                    model=st.session_state["openai_model"],
                    tools=[{"type": "code_interpreter", "container": st.session_state["container_id"]}],
                    input=messages
                )
                # Extract the answer from the response
                answer = extract_llm_response_code_interpreter(response.output)
                image = extract_llm_response_code_interpreter_image(response, st.session_state["container_id"])
                if answer == "":
                    raise ValueError("Empty answer received from LLM.")
            except Exception as e:
                print(f"Error processing question in container.\nError: {e}")
            print(f"Before cleaning: {answer}")
            response = latex_block_to_inline(str(answer))
            print(f"After cleaning: {response}")


            # Check if the response contains REFUSED=TRUE
            if "REFUSED=TRUE" in response:
                response = response.replace("REFUSED=TRUE", "")
                references = None  # No references to show if refused
            else:
                if "refused=false" in response.lower():
                    response = response.replace("REFUSED=FALSE", "")
                    response = response.replace("refused=false", "")
                if "refused = false" in response.lower():
                    response = response.replace("REFUSED = FALSE", "")
                    response = response.replace("refused = false", "") 
            with st.chat_message("assistant"):
                response = st.write_stream(stream_string_data(response))
                if image is not None:
                    st.image(image, caption="Generated image")#, use_column_width=True)
            st.session_state.messages_chat.append({"role": "assistant", "content": response})
            if image is not None:
                st.session_state.messages_chat.append({"role": "assistant", "content": image})
            if references:
                with st.chat_message("references", avatar="üìñ"):
                    if len(references) == 1:
                        ref = f"Reference: page {references[0]}"
                    else:
                        ref = f"References: pages {", ".join(references[:-1])}, and {references[-1]}"
                    st.write(ref)
                st.session_state.messages_chat.append({"role": "references", "content": ref})
            #print(st.session_state.messages_chat)

def latex_block_to_inline(text):
    # Replace LaTeX display math blocks (\[ ... \]) with inline math ($...$)
    # and join consecutive blocks with normal text in between
    # Also, replace [6pt] with '\\\\' and remove \; and \, for inline style
    def clean_latex(latex):
        latex = latex.replace("\n", "")
        latex = latex.replace("[6pt]", "\\\\")
        latex = latex.replace("\\;", "")
        latex = latex.replace("\\,", "")
        latex = latex.replace("$$", "$")  # Ensure double dollar signs are replaced with single
        return latex.strip()

    # Find all display math blocks
    blocks = re.split(r"\\\[(.*?)\\\]", text, flags=re.DOTALL)
    result = []
    for i, part in enumerate(blocks):
        if i % 2 == 0:
            # Outside math block, just strip and add as is
            if part.strip():
                result.append(part.strip())
        else:
            # Inside math block, convert to inline math
            latex = clean_latex(part)
            result.append(f"${latex}$")
    # Join with spaces, remove double spaces
    return " ".join(result).replace("  ", " ")

def generate_questions():
    
    # List of True/False questions with answers
    questions = [
        {
            "question": "If a square matrix A is invertible, then its determinant is zero.",
            "answer": False
        },
        {
            "question": "The transpose of a product of two matrices equals the product of their transposes in reverse order.",
            "answer": True
        },
        {
            "question": "All eigenvalues of an orthogonal matrix have absolute value 1.",
            "answer": True
        }
    ]
    return questions


def extract_llm_response_code_interpreter(response: list) -> str:
    output = ""
    #print(response)
    #print(len(response))
    for index in range(len(response)-1, -1, -1):
        if isinstance(response[index], ResponseOutputMessage):
            #print(len(response[out].content)) check if this length is 1
            #print(f"Output {index}:\n{response[index].content[0].text}")
            output = response[index].content[0].text
            break #stop after the answer is found
    return output

def extract_llm_response_code_interpreter_image(response: list, container_id) -> np.ndarray:
    save_as = None
    # Extract the file citation annotation from the last output message
    output_message = response.output[-1].content[0]
    annotations = output_message.annotations

    # Find the annotation with type 'container_file_citation'
    image_annotation = next(
        (ann for ann in annotations if getattr(ann, "type", "") == "container_file_citation"),
        None
    )

    if image_annotation is not None:
        file_id = image_annotation.file_id
        filename = image_annotation.filename
        print(f"Image file ID: {file_id}")
        print(f"Image filename: {filename}")

        # Check file extension
        if filename.lower().endswith(".png"):
            save_as = filename
        elif filename.lower().endswith(".jpg") or filename.lower().endswith(".jpeg"):
            save_as = filename
        else:
            raise ValueError("Unsupported file extension. Only .png and .jpg are supported.")

        # Download the file from the container
        url = f"https://api.openai.com/v1/containers/{container_id}/files/{file_id}/content"
        headers = {"Authorization": f"Bearer {OPENAI_API}"}

        response_file = requests.get(url, headers=headers)
        if response_file.status_code == 200:
            with open(save_as, "wb") as f:
                f.write(response_file.content)
            print(f"File downloaded and saved as {save_as}")
        else:
            print(f"Failed to download file: {response_file.status_code} - {response_file.text}")
    else:
        print("No image annotation found in the response.")
    return save_as

def practice_true_false_questions():
    questions = generate_questions()
    num_questions = len(questions)

    # === Session State Initialization ===
    if "question_index" not in st.session_state:
        st.session_state.question_index = 0
    if "submitted" not in st.session_state:
        st.session_state.submitted = [False] * num_questions
    if "user_answers" not in st.session_state:
        st.session_state.user_answers = [None] * num_questions
    if "show_score" not in st.session_state:
        st.session_state.show_score = False
    if "review_md" not in st.session_state:
        st.session_state.review_md = None

    def calculate_score():
        return sum(
            1 for i, q in enumerate(st.session_state.generated_questions)
            if st.session_state.user_answers[i] == q["answer"]
        )

    def generate_review_markdown():
        lines = ["# Quiz Review\n"]
        score = calculate_score()
        lines.append(f"## Final Score: {score} / {num_questions}")
        for i, q in enumerate(questions):
            user_ans = st.session_state.user_answers[i]
            correct = q["answer"]
            result = "‚úÖ Correct" if user_ans == correct else "‚ùå Incorrect"
            lines.append(f"### Question {i+1}: {q['question']}")
            #lines.append(f"**Q:** {q['question']}")
            lines.append(f"- Your answer: **{user_ans}**")
            lines.append(f"- Correct answer: **{correct}**")
            lines.append(f"- Result: {result}\n")

        return "\n".join(lines)


    def generate_review_pdf():
        pdf = FPDF()
        pdf.add_page()
        pdf.set_font("Arial", size=12)

        score = calculate_score()
        pdf.set_font("Arial", 'B', 14)
        pdf.cell(0, 10, "Quiz Review", ln=True)
        pdf.set_font("Arial", size=12)
        pdf.cell(0, 10, f"Final Score: {score} / {st.session_state.num_questions_final}", ln=True)
        #for i, q in enumerate()
        for i, q in enumerate(st.session_state.generated_questions):
            user_ans = st.session_state.user_answers[i]
            result = "Correct" if user_ans == q["answer"] else "Incorrect"
            pdf.ln(5)
            pdf.set_font("Arial", 'B', 12)
            pdf.multi_cell(0, 10, f"Question {i+1}: {q['question']}")
            pdf.set_font("Arial", size=12)
            pdf.cell(0, 10, f"- Your answer: {user_ans}", ln=True)
            pdf.cell(0, 10, f"- Correct answer: {q["answer"]}", ln=True)
            pdf.multi_cell(0, 10, f"- Explanation: {q['explanation']}")
            pdf.cell(0, 10, f"- Result: {result}", ln=True)

        # Get PDF as bytes
        pdf_str = str(pdf.output(dest='S'))
        pdf_bytes = pdf_str.encode('latin1')
        return io.BytesIO(pdf_bytes)

    # === Main UI ===
    st.write("# Practise True/False Questions")
    # Topic selection with radio buttons (max 5 active)
    topics = [
        "Systems of linear equations, Gaussian elimination",
        "Vector equations, Matrix",
        "Solution sets and Linear independence",
        "Linear Transformations, Matrix algebra",
        "The Inverse of a Matrix",
        "Determinants, Perspective projections",
        "Vector Spaces",
        "Eigenvalues and Eigenvectors",
        "Diagonalization",
        "Orthogonality and Symmetric Matrices"
    ]

    # Topic selection UI
    if "topics_submitted" not in st.session_state:
        st.session_state.topics_submitted = False

    if "generated_questions" not in st.session_state:
        st.session_state.generated_questions = None

    if not st.session_state.topics_submitted:
        st.write("The system will generate True/False questions about Linear Algebra. You can now choose which topics you want to practice.")
        selected_topics = st.multiselect(
            "Select up to 5 topics to practice:",
            topics,
            max_selections=5
        )
        if len(selected_topics) == 5:
            st.info("Maximum of 5 topics selected.")
        st.session_state.selected_topics = selected_topics
        
        # Slider for number of questions
        num_questions_slider = st.slider(
            "Select the number of questions to practice:",
            min_value=1,
            max_value=10,
            value=len(selected_topics),
            step=1
        )

        # Button to generate questions
        if st.button("Generate Questions"):
            if not selected_topics:
                st.warning("Please select at least one topic.")
            else:
                st.session_state.topics_submitted = True
                st.session_state.selected_topics_final = selected_topics
                st.session_state.num_questions_final = num_questions_slider
        st.stop()
    elif st.session_state.generated_questions is None:
        # Hide topic selection and slider after questions are generated
        with st.spinner("Generating questions, please wait..."):
            num_questions = str(st.session_state.num_questions_final)
            topics_str = ", ".join(st.session_state.selected_topics_final)
            custom_prompt = (
                "You are a tutor for generating True/False statements in Linear Algebra. "
                "Generate exactly " + num_questions + " True/False questions based on the selected topics. "
                "Please format as follows:\n"
                "- Each question should start with 'Q:' followed by the question text.\n"
                "- Each explanation should start with 'E:' followed by the explanation text.\n"
                "- Each answer should start with 'A:' followed by 'True' or 'False'.\n"
                "For example:\n"
                "Q: The determinant of a matrix is always non-negative.\n"
                "E: The determinant can be negative depending on the matrix. For example, $\\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix}$.\n"
                "A: False\n"
                "Please use LaTeX formatting for mathematical expressions by writing them between dollar signs."
                "For example, to write a matrix, use $\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$. "
                "You can write and run code to answer the question. "
                "\n\nSelected topics:\n" + topics_str
            )
            llm = OpenAI(api_key=OPENAI_API)

            if "openai_model" not in st.session_state:
                st.session_state["openai_model"] = MODEL_QUESTION_GENERATION
            if "container_id" not in st.session_state:
                print("Creating container for Code Interpreter...")
                cont = llm.containers.create(name="test")
                container_id_manual = cont.id
                st.session_state["container_id"] = container_id_manual

            try:
                response = llm.responses.create(
                    model=st.session_state["openai_model"],
                    tools=[{"type": "code_interpreter", "container": st.session_state["container_id"]}],
                    input=custom_prompt,
                )
                # Extract the answer from the response
                answer = extract_llm_response_code_interpreter(response.output)
                if answer == "":
                    raise ValueError("Empty answer received from LLM.")
            except Exception as e:
                print(f"Error generating questions.\nError: {e}")
                st.error(f"Error generating questions.\nError: {e}")

            # Use the correct message format for OpenAI
            #response = llm.chat.completions.create(
            #    model=st.session_state["openai_model"],
            #    messages= [{"role": "system","content": custom_prompt}],
            #    stream=False,
            #)
            #response_content = response.choices[0].message.content
            # Process the response to extract questions and answers
            response_content = answer
            questions = []
            explanations = []
            answers = []
            for line in response_content.split("\n"):
                line = line.strip()
                if line.startswith("Q:"):
                    question = line[2:].strip()
                    questions.append(question)
                    #questions.append(transform_latex_text2(question))
                elif line.startswith("E:"):
                    explanation = line[2:].strip()
                    explanations.append(explanation)
                    #explanations.append(transform_latex_text2(explanation))
                elif line.startswith("A:"):
                    answer = line[2:].strip().lower() == "true"
                    answers.append(answer)
            if len(questions) != len(answers) or len(questions) != st.session_state.num_questions_final:	
                st.error("Error: The number of questions and answers do not match the requested number.")
            time.sleep(1) # Simulate processing time
            # Store generated questions and answers in session state
            st.session_state.generated_questions = [
                {"question": q, "explanation": e, "answer": a} for q, e, a in zip(questions, explanations, answers)
            ]
            print(st.session_state.generated_questions)
            st.session_state.question_index = 0
            st.session_state.submitted = [False] * len(questions)
            st.session_state.user_answers = [None] * len(questions)
            st.session_state.show_score = False
            st.session_state.review_md = None

    # Display all questions with separate True/False buttons
    if st.session_state.generated_questions is not None:
        st.success("Questions generated!")
        st.markdown(
            f"**Selected topics:** {', '.join(st.session_state.selected_topics_final)}  \n"
            f"**Number of questions:** {st.session_state.num_questions_final}"
        )
        st.write("### Answer the following questions:")
        for idx, question_data in enumerate(st.session_state.generated_questions):
            #st.markdown(f"**Question {idx + 1}:** {question_data['question']}")
            st.write(f"**Question {idx + 1}:** {question_data['question']}")
            col1, col2, col3 = st.columns([1, 1, 1])
            help_key = f"help_active_{idx}"
            with col1:
                if st.button(f"True", key=f"true_btn_{idx}", disabled=st.session_state.submitted[idx]):
                    user_answers = st.session_state.user_answers
                    user_answers[idx] = True
                    st.session_state.user_answers = user_answers
                    submitted = st.session_state.submitted
                    submitted[idx] = True
                    st.session_state.submitted = submitted
                    # Reset help state for this question
                    st.session_state[help_key] = False
            with col2:
                if st.button(f"False", key=f"false_btn_{idx}", disabled=st.session_state.submitted[idx]):
                    user_answers = st.session_state.user_answers
                    user_answers[idx] = False
                    st.session_state.user_answers = user_answers
                    submitted = st.session_state.submitted
                    submitted[idx] = True
                    st.session_state.submitted = submitted
                    # Reset help state for this question
                    st.session_state[help_key] = False
            # Use a session state key to track which help button was pressed
            with col3:
                if st.button("Help", key=f"help_btn_{idx}"):
                    # Set the help key for this question to True, others to False
                    for i in range(len(st.session_state.generated_questions)):
                        st.session_state[f"help_active_{i}"] = (i == idx)
            # Show chat across all columns if help is active for this question
            if st.session_state.get(help_key, False) and not st.session_state.submitted[idx]:
                #st.markdown("---")
                chat_TF_generation(question_data, idx=idx)
                st.markdown("---")
            # Show feedback if answered
            if st.session_state.submitted[idx]:
                correct_answer = question_data['answer']
                explanation = question_data['explanation']
                user_answer = st.session_state.user_answers[idx]
                if user_answer == correct_answer:
                    st.success(f"Question {idx + 1}: Correct! üéâ\n\nExplanation: {explanation}")
                else:
                    st.error(f"Question {idx + 1}: Incorrect. The correct answer is {'True' if correct_answer else 'False'}.\n\nExplanation: {explanation}")

        # Optionally, show score or review after all questions are answered
        if all(st.session_state.submitted):
            score = sum(
                1 for i, q in enumerate(st.session_state.generated_questions)
                if st.session_state.user_answers[i] == q["answer"]
            )
            st.info(f"Quiz complete! Your score: {score} / {len(st.session_state.generated_questions)}")

            ## PDF option
            pdf_data = generate_review_pdf()
            st.download_button(
                label="Download Review as PDF",
                data=pdf_data,
                file_name="quiz_review.pdf",
                mime="application/pdf"
            )
            if st.button("Reset and Start Again"):
                for key in [
                    "topics_submitted",
                    "selected_topics",
                    "selected_topics_final",
                    "num_questions_final",
                    #"generated_questions",
                    "question_index",
                    "submitted",
                    "user_answers",
                    "show_score",
                    "review_md",
                ]:
                    if key in st.session_state:
                        del st.session_state[key]

                for idx, question_data in enumerate(st.session_state.generated_questions):
                    if f"messages_{idx}" in st.session_state:
                        safe_time = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
                        with open(f'chats/TF_chat_{idx}_list_{safe_time}.pkl', 'wb') as f:
                            pickle.dump(st.session_state[f"messages_{idx}"], f)
                        del st.session_state[f"messages_{idx}"]
                    if f"init_chat_TF_{idx}" in st.session_state:
                        del st.session_state[f"init_chat_TF_{idx}"]
                    if f"chat_{idx}" in st.session_state:
                        del st.session_state[f"chat_{idx}"]
                if "generated_questions" in st.session_state:
                    del st.session_state["generated_questions"]
                st.rerun()
        

page_names_to_funcs = {
    "Main menu": intro,
    #"Linear Algebra chat": chat_TF_generation,
    "Linear Algebra chat": chat,
    "Practise T/F questions": practice_true_false_questions,
}

demo_name = st.sidebar.selectbox("Select a practice mode:", page_names_to_funcs.keys())
page_names_to_funcs[demo_name]()